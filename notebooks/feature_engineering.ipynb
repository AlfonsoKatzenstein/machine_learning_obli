{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec455bcf",
   "metadata": {},
   "source": [
    "# Feature Engineering - Proyecto ML\n",
    "\n",
    "Este notebook contiene el proceso completo de ingeniería de características:\n",
    "1. **Manejo de variables categóricas** (alta cardinalidad)\n",
    "2. **Creación de nuevas características**\n",
    "3. **Selección de características**\n",
    "4. **Gestión de data leakage** con pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90e7b7",
   "metadata": {},
   "source": [
    "## 1. Configuración y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Rutas del proyecto\n",
    "PROJ = Path.cwd() if (Path.cwd().name != \"notebooks\") else Path.cwd().parent\n",
    "DATA = PROJ / \"data\"\n",
    "\n",
    "# Cargar datos\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "test = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nColumnas: {train.columns.tolist()}\")\n",
    "print(f\"\\nTarget variable: popularity\")\n",
    "print(f\"  - Min: {train['popularity'].min()}\")\n",
    "print(f\"  - Max: {train['popularity'].max()}\")\n",
    "print(f\"  - Mean: {train['popularity'].mean():.2f}\")\n",
    "print(f\"  - Std: {train['popularity'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e03d32",
   "metadata": {},
   "source": [
    "## 2. Análisis de Variables Categóricas - Alta Cardinalidad\n",
    "\n",
    "**Problema:** Variables como `artists`, `album_name`, `track_name` tienen miles de valores únicos (alta cardinalidad).\n",
    "\n",
    "**Estrategias:**\n",
    "- **Frequency Encoding:** Reemplazar por frecuencia de aparición\n",
    "- **Target Encoding:** Reemplazar por promedio del target (con validación cruzada para evitar leakage)\n",
    "- **Agrupación:** Agrupar categorías poco frecuentes en \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba114bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variables categóricas y su cardinalidad\n",
    "cat_cols = train.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(\"ANÁLISIS DE CARDINALIDAD:\")\n",
    "print(\"=\" * 60)\n",
    "for col in cat_cols:\n",
    "    n_unique_train = train[col].nunique()\n",
    "    n_unique_test = test[col].nunique()\n",
    "    print(f\"{col:20s}: {n_unique_train:6d} únicos (train), {n_unique_test:6d} únicos (test)\")\n",
    "    \n",
    "    # Calcular % de valores que aparecen solo 1 vez\n",
    "    value_counts = train[col].value_counts()\n",
    "    singleton_pct = (value_counts == 1).sum() / len(value_counts) * 100\n",
    "    print(f\"  {'':20s}  {singleton_pct:.1f}% aparecen solo 1 vez\")\n",
    "    \n",
    "    # Mostrar top 5 más frecuentes\n",
    "    print(f\"  Top 5: {value_counts.head(5).to_dict()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c68f7c",
   "metadata": {},
   "source": [
    "### 2.1 Transformadores Personalizados para Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Codifica variables categóricas por su frecuencia de aparición.\n",
    "    \n",
    "    Ventajas:\n",
    "    - Maneja alta cardinalidad sin crear miles de columnas\n",
    "    - No genera data leakage (aprende solo de train)\n",
    "    - Valores desconocidos en test reciben frecuencia 0\n",
    "    \"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.frequency_maps = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                # Calcular frecuencias solo con train\n",
    "                freq_map = X[col].value_counts(normalize=True).to_dict()\n",
    "                self.frequency_maps[col] = freq_map\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_copy.columns and col in self.frequency_maps:\n",
    "                # Mapear, valores desconocidos → 0\n",
    "                X_copy[f'{col}_freq'] = X_copy[col].map(self.frequency_maps[col]).fillna(0)\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Codifica variables categóricas por el promedio del target.\n",
    "    \n",
    "    IMPORTANTE: Usa smoothing para evitar overfitting en categorías raras.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns, smoothing=10):\n",
    "        self.columns = columns\n",
    "        self.smoothing = smoothing\n",
    "        self.target_maps = {}\n",
    "        self.global_mean = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.global_mean = y.mean()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                # Calcular promedio por categoría con smoothing\n",
    "                agg = X[[col]].copy()\n",
    "                agg['target'] = y\n",
    "                \n",
    "                stats = agg.groupby(col)['target'].agg(['mean', 'count'])\n",
    "                \n",
    "                # Smoothing: mezcla promedio de categoría con promedio global\n",
    "                # Fórmula: (count * mean + smoothing * global_mean) / (count + smoothing)\n",
    "                smoothed_mean = (\n",
    "                    (stats['count'] * stats['mean'] + self.smoothing * self.global_mean) /\n",
    "                    (stats['count'] + self.smoothing)\n",
    "                )\n",
    "                \n",
    "                self.target_maps[col] = smoothed_mean.to_dict()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X_copy.columns and col in self.target_maps:\n",
    "                # Mapear, valores desconocidos → global mean\n",
    "                X_copy[f'{col}_target_enc'] = X_copy[col].map(self.target_maps[col]).fillna(self.global_mean)\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "print(\"✓ Transformadores creados:\")\n",
    "print(\"  - FrequencyEncoder: Codifica por frecuencia\")\n",
    "print(\"  - TargetEncoder: Codifica por promedio del target (con smoothing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78377d6",
   "metadata": {},
   "source": [
    "## 3. Creación de Nuevas Características\n",
    "\n",
    "Crearemos features que capturen información relevante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Crea nuevas características a partir de las existentes.\n",
    "    \n",
    "    Features creadas:\n",
    "    1. duration_minutes: Duración en minutos (más interpretable)\n",
    "    2. energy_danceability: Interacción entre energía y bailabilidad\n",
    "    3. acousticness_instrumentalness: Ratio acústico/instrumental\n",
    "    4. loudness_energy_ratio: Relación loudness/energy\n",
    "    5. speechiness_high: Flag de alto contenido hablado (>0.66)\n",
    "    6. is_explicit_int: Conversión de explicit a numérico\n",
    "    7. track_name_length: Largo del nombre de la canción\n",
    "    8. artists_count: Número de artistas (si hay múltiples separados por coma/feat)\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # 1. Duración en minutos (más interpretable que milisegundos)\n",
    "        if 'duration_ms' in X_new.columns:\n",
    "            X_new['duration_minutes'] = X_new['duration_ms'] / 60000\n",
    "        \n",
    "        # 2. Interacción entre energy y danceability\n",
    "        # Canciones \"party\" suelen tener ambas altas\n",
    "        if 'energy' in X_new.columns and 'danceability' in X_new.columns:\n",
    "            X_new['energy_danceability'] = X_new['energy'] * X_new['danceability']\n",
    "        \n",
    "        # 3. Ratio acústico/instrumental\n",
    "        # Canciones muy acústicas vs muy instrumentales\n",
    "        if 'acousticness' in X_new.columns and 'instrumentalness' in X_new.columns:\n",
    "            X_new['acoustic_instrumental_ratio'] = (\n",
    "                X_new['acousticness'] / (X_new['instrumentalness'] + 0.01)  # +0.01 para evitar división por 0\n",
    "            )\n",
    "        \n",
    "        # 4. Relación loudness/energy\n",
    "        # Canciones que suenan fuerte pero tienen poca energía (o viceversa)\n",
    "        if 'loudness' in X_new.columns and 'energy' in X_new.columns:\n",
    "            # Normalizar loudness a [0,1] primero (suele estar en [-60, 0])\n",
    "            loudness_norm = (X_new['loudness'] + 60) / 60\n",
    "            X_new['loudness_energy_ratio'] = loudness_norm / (X_new['energy'] + 0.01)\n",
    "        \n",
    "        # 5. Flag de alto contenido hablado (podcast, spoken word)\n",
    "        if 'speechiness' in X_new.columns:\n",
    "            X_new['speechiness_high'] = (X_new['speechiness'] > 0.66).astype(int)\n",
    "        \n",
    "        # 6. Explicit como numérico (si existe)\n",
    "        if 'explicit' in X_new.columns:\n",
    "            X_new['is_explicit_int'] = X_new['explicit'].astype(int)\n",
    "        \n",
    "        # 7. Largo del nombre de la canción\n",
    "        if 'track_name' in X_new.columns:\n",
    "            X_new['track_name_length'] = X_new['track_name'].fillna('').astype(str).str.len()\n",
    "        \n",
    "        # 8. Número de artistas (detectar colaboraciones)\n",
    "        if 'artists' in X_new.columns:\n",
    "            # Contar separadores: ',' y 'feat' indican múltiples artistas\n",
    "            X_new['artists_count'] = (\n",
    "                X_new['artists'].fillna('').astype(str).str.count(',') + \n",
    "                X_new['artists'].fillna('').astype(str).str.lower().str.count('feat') + 1\n",
    "            )\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "\n",
    "feature_creator = FeatureCreator()\n",
    "print(\"✓ FeatureCreator listo\")\n",
    "print(\"\\nNuevas features que se crearán:\")\n",
    "print(\"  1. duration_minutes - Duración en minutos\")\n",
    "print(\"  2. energy_danceability - Interacción energy × danceability\") \n",
    "print(\"  3. acoustic_instrumental_ratio - Ratio acústico/instrumental\")\n",
    "print(\"  4. loudness_energy_ratio - Relación loudness/energy\")\n",
    "print(\"  5. speechiness_high - Flag de alto contenido hablado (>0.66)\")\n",
    "print(\"  6. is_explicit_int - Explicit como numérico\")\n",
    "print(\"  7. track_name_length - Largo del nombre\")\n",
    "print(\"  8. artists_count - Número de artistas (colaboraciones)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34067991",
   "metadata": {},
   "source": [
    "## 4. Selección de Características - Eliminación de Columnas Irrelevantes\n",
    "\n",
    "**Criterios para eliminar columnas:**\n",
    "1. **Identificadores únicos** (track_id, track_name) - No aportan información predictiva\n",
    "2. **Alta cardinalidad sin patrón** - Ya las codificamos, podemos eliminar las originales\n",
    "3. **Información redundante** - Ya capturada en otras features\n",
    "4. **Leakage potencial** - Información que no estaría disponible antes de la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Elimina columnas irrelevantes o problemáticas.\n",
    "    \n",
    "    Columnas a eliminar:\n",
    "    - track_id: Identificador único, no predictivo\n",
    "    - track_name: Ya capturamos su largo en track_name_length\n",
    "    - artists: Ya codificada con frequency/target encoding\n",
    "    - album_name: Ya codificada con frequency/target encoding\n",
    "    - explicit: Ya convertida a is_explicit_int\n",
    "    - duration_ms: Ya convertida a duration_minutes (más interpretable)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # Solo eliminar columnas que existen\n",
    "        existing_cols = [col for col in self.columns_to_drop if col in X_copy.columns]\n",
    "        if existing_cols:\n",
    "            X_copy = X_copy.drop(columns=existing_cols)\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# Definir qué columnas eliminar\n",
    "columns_to_drop = [\n",
    "    'track_id',      # Identificador único\n",
    "    'track_name',    # Ya usamos track_name_length\n",
    "    'artists',       # Ya codificada\n",
    "    'album_name',    # Ya codificada\n",
    "    'explicit',      # Ya convertida a is_explicit_int\n",
    "    'duration_ms'    # Ya convertida a duration_minutes\n",
    "]\n",
    "\n",
    "print(\"JUSTIFICACIÓN DE ELIMINACIÓN DE COLUMNAS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✗ track_id: Identificador único, no aporta información predictiva\")\n",
    "print(\"✗ track_name: Ya capturamos su información en track_name_length\")\n",
    "print(\"✗ artists: Ya codificada con frequency/target encoding + artists_count\")\n",
    "print(\"✗ album_name: Ya codificada con frequency/target encoding\")\n",
    "print(\"✗ explicit: Ya convertida a feature numérica is_explicit_int\")\n",
    "print(\"✗ duration_ms: Redundante, usamos duration_minutes (más interpretable)\")\n",
    "print(\"\\n✓ Estas columnas se eliminarán DESPUÉS de crear las features derivadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4df925",
   "metadata": {},
   "source": [
    "## 5. Transformación de Variables Asimétricas (Skewness)\n",
    "\n",
    "Aplicar transformación logarítmica a variables con distribución asimétrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32141863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero debemos aplicar feature_creator para tener todas las columnas numéricas\n",
    "temp_train = feature_creator.fit_transform(train)\n",
    "\n",
    "# Identificar variables numéricas continuas\n",
    "num_continuous = temp_train.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'popularity' in num_continuous:\n",
    "    num_continuous.remove('popularity')\n",
    "\n",
    "# Analizar skewness\n",
    "print(\"ANÁLISIS DE ASIMETRÍA (SKEWNESS):\")\n",
    "print(\"=\" * 60)\n",
    "skewness_dict = {}\n",
    "for col in num_continuous:\n",
    "    skew_value = temp_train[col].skew()\n",
    "    skewness_dict[col] = skew_value\n",
    "    \n",
    "    print(f\"{col:30s}: skew = {skew_value:7.2f}\", end=\"\")\n",
    "    if abs(skew_value) > 1:\n",
    "        print(\"  --> MUY ASIMÉTRICA ⚠️\")\n",
    "    elif abs(skew_value) > 0.5:\n",
    "        print(\"  --> Asimetría moderada\")\n",
    "    else:\n",
    "        print(\"  --> Simétrica ✓\")\n",
    "\n",
    "# Identificar columnas que necesitan transformación log\n",
    "skewed_cols = [\n",
    "    col for col in num_continuous \n",
    "    if abs(skewness_dict[col]) > 1.0 and (temp_train[col] >= 0).all()\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Columnas seleccionadas para transformación logarítmica:\")\n",
    "for col in skewed_cols:\n",
    "    print(f\"  - {col} (skewness = {skewness_dict[col]:.2f})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Aplica transformación logarítmica a columnas específicas.\n",
    "    Guarda las columnas internamente para evitar problemas de scope.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns_to_transform):\n",
    "        self.columns_to_transform = columns_to_transform\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns_to_transform:\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = np.log1p(X_copy[col])\n",
    "        return X_copy\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns_to_transform:\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = np.expm1(X_copy[col])\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "log_transformer = LogTransformer(columns_to_transform=skewed_cols)\n",
    "print(f\"✓ LogTransformer creado para {len(skewed_cols)} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b992aa0d",
   "metadata": {},
   "source": [
    "## 6. Pipeline Completo - EVITANDO DATA LEAKAGE\n",
    "\n",
    "**Orden crítico para evitar leakage:**\n",
    "1. Crear nuevas features (FeatureCreator)\n",
    "2. Codificar categóricas con info del train (FrequencyEncoder, TargetEncoder)\n",
    "3. Transformar variables asimétricas (LogTransformer)\n",
    "4. Eliminar columnas originales (FeatureSelector)\n",
    "5. Imputar valores faltantes (SimpleImputer)\n",
    "6. Escalar variables numéricas (RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Identificar columnas categóricas de alta cardinalidad para codificar\n",
    "high_cardinality_cols = ['artists', 'album_name']\n",
    "\n",
    "# Identificar columnas categóricas de baja cardinalidad para OneHotEncoding\n",
    "# (si existen otras categóricas que no sean de alta cardinalidad)\n",
    "low_cardinality_cols = [col for col in cat_cols if col not in high_cardinality_cols \n",
    "                        and col not in ['track_id', 'track_name']]\n",
    "\n",
    "print(\"CONFIGURACIÓN DEL PIPELINE:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Alta cardinalidad (Frequency + Target Encoding): {high_cardinality_cols}\")\n",
    "print(f\"Baja cardinalidad (OneHotEncoding): {low_cardinality_cols}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pipeline de preprocesamiento que se aplica ANTES del ColumnTransformer\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "    ('feature_creator', feature_creator),\n",
    "    ('freq_encoder', FrequencyEncoder(columns=high_cardinality_cols)),\n",
    "    ('log_transformer', log_transformer),\n",
    "])\n",
    "\n",
    "# Después de feature engineering, necesitamos saber qué columnas son numéricas\n",
    "# Para esto, hacemos un fit_transform temporal\n",
    "temp_transformed = feature_engineering_pipeline.fit_transform(train, train['popularity'])\n",
    "\n",
    "# Identificar columnas numéricas después de feature engineering\n",
    "numeric_features = temp_transformed.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'popularity' in numeric_features:\n",
    "    numeric_features.remove('popularity')\n",
    "\n",
    "# Identificar columnas categóricas restantes\n",
    "categorical_features = temp_transformed.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(f\"\\nDespués de Feature Engineering:\")\n",
    "print(f\"  - Features numéricas: {len(numeric_features)}\")\n",
    "print(f\"  - Features categóricas: {len(categorical_features)}\")\n",
    "\n",
    "# Pipeline numérico: imputación + escalado\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())  # Robusto a outliers\n",
    "])\n",
    "\n",
    "# Pipeline categórico: imputación + one-hot encoding\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# ColumnTransformer para aplicar diferentes transformaciones\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Pipeline completo final\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_engineering', feature_engineering_pipeline),\n",
    "    ('feature_selector', FeatureSelector(columns_to_drop=columns_to_drop)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n✓ Pipeline completo creado\")\n",
    "print(\"\\nORDEN DE TRANSFORMACIONES (sin data leakage):\")\n",
    "print(\"  1. FeatureCreator - Crear nuevas features\")\n",
    "print(\"  2. FrequencyEncoder - Codificar alta cardinalidad\")\n",
    "print(\"  3. LogTransformer - Transformar variables asimétricas\")\n",
    "print(\"  4. FeatureSelector - Eliminar columnas irrelevantes\")\n",
    "print(\"  5. Preprocessor:\")\n",
    "print(\"     - Numéricas: Imputar (mediana) + Escalar (RobustScaler)\")\n",
    "print(\"     - Categóricas: Imputar (Unknown) + OneHotEncode\")\n",
    "print(\"  6. RandomForestRegressor - Modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439429ee",
   "metadata": {},
   "source": [
    "## 7. Entrenamiento y Validación\n",
    "\n",
    "Entrenamos el pipeline completo con validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2595db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import time\n",
    "\n",
    "# Separar X e y\n",
    "X_train_full = train.drop(columns=['popularity'])\n",
    "y_train_full = train['popularity']\n",
    "\n",
    "# Split para validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"ENTRENAMIENTO DEL MODELO:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {test.shape}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Entrenar el pipeline completo\n",
    "start_time = time.time()\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {training_time:.2f} segundos\")\n",
    "\n",
    "# Predecir en validación\n",
    "y_val_pred = full_pipeline.predict(X_val)\n",
    "\n",
    "# Métricas\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nMÉTRICAS EN VALIDACIÓN:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"RMSE: {rmse_val:.4f}\")\n",
    "print(f\"MAE:  {mae_val:.4f}\")\n",
    "print(f\"R²:   {r2_val:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218de2b",
   "metadata": {},
   "source": [
    "## 8. Validación Cruzada (Cross-Validation)\n",
    "\n",
    "Para asegurar que el modelo generaliza bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada con 5 folds\n",
    "print(\"VALIDACIÓN CRUZADA (5-Fold):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    full_pipeline, \n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_rmse_scores = np.sqrt(-cv_scores)\n",
    "\n",
    "print(f\"RMSE por fold:\")\n",
    "for i, score in enumerate(cv_rmse_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nPromedio: {cv_rmse_scores.mean():.4f} (+/- {cv_rmse_scores.std():.4f})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualizar resultados de CV\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.boxplot(cv_rmse_scores, labels=['5-Fold CV'])\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Distribución de RMSE en Validación Cruzada')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee84fca",
   "metadata": {},
   "source": [
    "## 9. Análisis de Importancia de Features\n",
    "\n",
    "Verificar qué features son más importantes para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener importancia de features del modelo entrenado\n",
    "model = full_pipeline.named_steps['model']\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Obtener nombres de features después de la transformación\n",
    "# Esto es complicado porque ColumnTransformer cambia los nombres\n",
    "# Vamos a reconstruirlos manualmente\n",
    "\n",
    "# Aplicar solo el preprocesamiento (sin el modelo)\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('feature_engineering', feature_engineering_pipeline),\n",
    "    ('feature_selector', FeatureSelector(columns_to_drop=columns_to_drop)),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# Obtener nombres de features numéricas (después del preprocesamiento)\n",
    "numeric_feature_names = [col for col in numeric_features if col not in columns_to_drop]\n",
    "\n",
    "# Nombres de features categóricas después de OneHotEncoding\n",
    "categorical_feature_names = []\n",
    "if len(categorical_features) > 0:\n",
    "    # El OneHotEncoder genera nombres automáticamente\n",
    "    # Necesitamos obtenerlos del preprocessor\n",
    "    try:\n",
    "        ohe = full_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "        categorical_feature_names = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Combinar nombres\n",
    "all_feature_names = numeric_feature_names + categorical_feature_names\n",
    "\n",
    "# Si hay diferencia en la longitud, usar nombres genéricos\n",
    "if len(all_feature_names) != len(feature_importances):\n",
    "    all_feature_names = [f'feature_{i}' for i in range(len(feature_importances))]\n",
    "\n",
    "# Crear DataFrame de importancia\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Mostrar top 20\n",
    "print(\"TOP 20 FEATURES MÁS IMPORTANTES:\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "top_features = feature_importance_df.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['importance'])\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Importancia')\n",
    "ax.set_title('Top 20 Features Más Importantes')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd5978",
   "metadata": {},
   "source": [
    "## 10. Predicción en el Conjunto de Test\n",
    "\n",
    "Finalmente, aplicamos el pipeline al conjunto de test (sin leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bafbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-entrenar con TODO el conjunto de train\n",
    "print(\"ENTRENAMIENTO FINAL CON TODO EL CONJUNTO DE TRAIN:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "full_pipeline.fit(X_train_full, y_train_full)\n",
    "print(\"✓ Modelo entrenado con todos los datos de train\")\n",
    "\n",
    "# Predecir en test\n",
    "y_test_pred = full_pipeline.predict(test)\n",
    "\n",
    "print(f\"\\n✓ Predicciones generadas: {len(y_test_pred)}\")\n",
    "print(f\"  Rango: [{y_test_pred.min():.2f}, {y_test_pred.max():.2f}]\")\n",
    "print(f\"  Media: {y_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {y_test_pred.std():.2f}\")\n",
    "\n",
    "# Comparar con distribución de train\n",
    "print(f\"\\nDistribución de popularity en train:\")\n",
    "print(f\"  Rango: [{y_train_full.min():.2f}, {y_train_full.max():.2f}]\")\n",
    "print(f\"  Media: {y_train_full.mean():.2f}\")\n",
    "print(f\"  Std: {y_train_full.std():.2f}\")\n",
    "\n",
    "# Visualizar distribuciones\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(y_train_full, bins=50, alpha=0.7, label='Train (real)', edgecolor='black')\n",
    "axes[0].set_xlabel('Popularity')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].set_title('Distribución de Popularity - Train')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(y_test_pred, bins=50, alpha=0.7, label='Test (predicciones)', \n",
    "             color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Popularity')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_title('Distribución de Predicciones - Test')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ PROCESO COMPLETADO SIN DATA LEAKAGE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
